## Python3多进程

&emsp;&emsp;Unix/Linux操作系统提供了一个`fork()`系统调用，它非常特殊。普通的函数调用，调用
一次，返回一次，但是`fork()`调用一次，返回两次，因为操作系统自动把当前进程（称为父进程）复制了
一份（称为子进程），然后，分别在父进程和子进程内返回。  
&emsp;&emsp;子进程永远返回0，而父进程返回子进程的ID。这样做的理由是，一个父进程可以`fork`出很
多子进程，所以，父进程要记下每个子进程的ID，而子进程只需要调用getppid()就可以拿到父进程的ID。  
&emsp;&emsp;Python的`os`模块封装了常见的系统调用，其中就包括`fork`，可以在Python程序中轻松
创建子进程：

```python
# -*- coding: utf-8 -*-
# filename: process.py

import time
import os


# 子进程要执行的代码
def task(name):
    print('Child process (%d) start...' % os.getpid())
    print('Child process %s run.' % name)
    time.sleep(1)
    print('Child process end.')


print('Parent Process (%s) start...' % os.getpid())
# Only works on Unix/Linux/Mac:
pid = os.fork()
if pid == 0:
    task('test')
else:
    print('Parent process run.')
    time.sleep(1)
    print('Parent process end.')
```

执行以上程序输出结果如下：

```sh
$ python3 process.py
Parent Process (2934) start...
Parent process run.
Child process (2935) start...
Child process test run.
Child process end.
Parent process end.
```

&emsp;&emsp;由于Windows没有fork调用，上面的代码在Windows上无法运行。由于Mac系统是基于BSD
（Unix的一种）内核，所以，在Mac下运行是没有问题的，推荐大家用Mac学Python！  

&emsp;&emsp;有了`fork`调用，一个进程在接到新任务时就可以复制出一个子进程来处理新任务，常见的
Apache服务器就是由父进程监听端口，每当有新的http请求时，就`fork`出子进程来处理新的http请求。

### 多进程(`multiprocessing`)

&emsp;&emsp;如果你打算编写多进程的服务程序，Unix/Linux无疑是正确的选择。由于Windows没有
fork调用，难道在Windows上无法用Python编写多进程的程序？  
&emsp;&emsp;由于Python是跨平台的，自然也应该提供一个跨平台的多进程支持。`multiprocessing`
模块就是跨平台版本的多进程模块。  
&emsp;&emsp;`multiprocessing`模块提供了一个`Process`类来代表一个进程对象，下面的例子演示
了启动一个子进程并等待其结束：

```python
# -*- coding: utf-8 -*-
# filename: process.py

from multiprocessing import Process
import time
import os


# 子进程要执行的代码
def task(name):
    print('Child process (%d) start...' % os.getpid())
    print('Child process %s run.' % name)
    time.sleep(1)
    print('Child process end.')


if __name__ == '__main__':
    print('Parent process (%s) start...' % os.getpid())

    process = Process(target=task, args=('test',))
    process.start()
    print('Child process (%d) create success.' % process.pid)

    print('Parent process run.')
    time.sleep(1)

    process.join()

    print('Parent process end.')
```

执行结果如下：

```sh
$ python3 process.py
Parent process (2969) start...
Child process (2970) create success.
Parent process run.
Child process (2970) start...
Child process test run.
Child process end.
Parent process end.
```

&emsp;&emsp;创建子进程时，只需要传入一个执行函数和函数的参数，创建一个Process实例，用
`start()`方法启动，这样创建进程比`fork()`还要简单。  
&emsp;&emsp;join()方法可以等待子进程结束后再继续往下运行，通常用于进程间的同步。  

### 进程池(`Pool`)

&emsp;&emsp;如果要启动大量的子进程，可以用进程池的方式批量创建子进程：

```python
# -*- coding: utf-8 -*-
# filename: process.py

from multiprocessing import Pool
import random
import time
import os


def long_time_task(name):
    print('Child process %s start...' % name)
    print('Child process %s run.' % name)
    start = time.time()
    time.sleep(random.random() * 5)
    print('Child process %s runs %0.2f seconds.' % (name, (time.time() - start)))
    print('Child process %s end.' % name)


if __name__ == '__main__':
    print('Parent process (%s) start...' % os.getpid())

    pool = Pool(3)
    for i in range(5):
        pool.apply_async(long_time_task, args=(i + 1,))

    print('Waiting for all subprocesses done...')
    pool.close()
    pool.join()
    print('All subprocesses done.')
    print('Parent process end.')
```

执行结果如下：

```sh
$ python3 process.py
Parent process (3246) start...
Waiting for all subprocesses done...
Child process 1 start...
Child process 1 run.
Child process 2 start...
Child process 2 run.
Child process 3 start...
Child process 3 run.
Child process 1 runs 0.27 seconds.
Child process 1 end.
Child process 4 start...
Child process 4 run.
Child process 4 runs 1.00 seconds.
Child process 4 end.
Child process 5 start...
Child process 5 run.
Child process 3 runs 1.65 seconds.
Child process 3 end.
Child process 2 runs 2.48 seconds.
Child process 2 end.
Child process 5 runs 2.15 seconds.
Child process 5 end.
All subprocesses done.
Parent process end.
```

**代码解读**：

&emsp;&emsp;Pool对象调用`join()`方法会等待所有子进程执行完毕，调用`join()`之前必须先调用
`close()`，调用`close()`之后就不能继续添加新的Process了。  
&emsp;&emsp;请注意输出的结果，task 1，2，3是立刻执行的，而task 4, 5要等待前面某个task完成
后才执行，这是因为Pool的大小为3，因此，最多同时执行3个进程。这是Pool有意设计的限制，并不是操作
系统的限制。如果改成：`pool = Pool(5)`就可以同时跑5个进程。  
&emsp;&emsp;Pool的默认大小是CPU的核数，如果你的CPU有8个核心，但要提交至少9个子进程才能看到上
面的等待效果。

### 子进程

&emsp;&emsp;很多时候，子进程并不是自身，而是一个外部进程。我们创建了子进程后，还需要控制子进程
的输入和输出。  
&emsp;&emsp;`subprocess`模块可以让我们非常方便地启动一个子进程，然后控制其输入和输出。  
&emsp;&emsp;下面的例子演示了如何在Python代码中运行命令`nslookup www.python.org`，这和命
令行直接运行的效果是一样的：

```python
# -*- coding: utf-8 -*-
# filename: process.py

import subprocess

print('$ nslookup www.python.org')
r = subprocess.call(['nslookup', 'www.python.org'])
print('Exit code:', r)
```

运行结果：

```sh
$ python3 process.py
$ nslookup www.python.org
Server:     192.168.105.104
Address:    192.168.105.104#53

Non-authoritative answer:
www.python.org  canonical name = dualstack.python.map.fastly.net.
Name:   dualstack.python.map.fastly.net
Address: 151.101.108.223

Exit code: 0
```

&emsp;&emsp;如果子进程还需要输入，则可以通过`communicate()`方法输入：

```python
# -*- coding: utf-8 -*-
# filename: process.py

import subprocess

print('$ nslookup')
p = subprocess.Popen(['nslookup'],
        stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
output, err = p.communicate(b'set q=mx\npython.org\nexit\n')
print(output.decode('utf-8'))
print('Exit code:', p.returncode)
```

上面的代码相当于在终端执行如下操作：

```sh
$ nslookup
> set q=mx
> python.org
Server:     192.168.105.104
Address:    192.168.105.104#53

Non-authoritative answer:
python.org  mail exchanger = 50 mail.python.org.

Authoritative answers can be found from:
> exit
```

运行结果如下：

```sh
$ python3 process.py
$ nslookup
Server:     192.168.105.104
Address:    192.168.105.104#53

Non-authoritative answer:
python.org  mail exchanger = 50 mail.python.org.

Authoritative answers can be found from:
mail.python.org internet address = 188.166.95.178
mail.python.org has AAAA address 2a03:b0c0:2:d0::71:1


Exit code: 0
```

### 进程间通信

&emsp;&emsp;Process之间肯定是需要通信的，操作系统提供了很多机制来实现进程间的通信。Python的
`multiprocessing`模块包装了底层的机制，提供了`Queue`、`Pipes`等多种方式来交换数据。  
&emsp;&emsp;我们以`Queue`为例，在父进程中创建两个子进程，一个往`Queue`里写数据，一个从
`Queue`里读数据：  

```python
# -*- coding: utf-8 -*-
# filename: process.py

from multiprocessing import Process, Queue
import random
import time
import os


# 写数据进程执行的代码:
def write(queue):
    print('Process to write: %s' % os.getpid())
    for value in ['A', 'B', 'C']:
        print('Put %s to queue...' % value)
        queue.put(value)
        time.sleep(random.random())


# 读数据进程执行的代码:
def read(q):
    print('Process to read: %s' % os.getpid())
    while True:
        value = q.get(True)
        print('Get %s from queue.' % value)


if __name__ == '__main__':
    # 父进程创建Queue，并传给各个子进程：
    queue = Queue()
    process_write = Process(target=write, args=(queue,))
    process_read = Process(target=read, args=(queue,))
    # 启动写数据子进程，写入:
    process_write.start()
    # 启动读书节子进程，读取:
    process_read.start()
    # 等待写数据子进程结束:
    process_write.join()
    # 读数据子进程里是死循环，无法等待其结束，只能强行终止:
    process_read.terminate()
```

运行结果如下：

```sh
$ python3 process.py
Process to write: 3387
Put A to queue...
Process to read: 3388
Get A from queue.
Put B to queue...
Get B from queue.
Put C to queue...
Get C from queue.
```

&emsp;&emsp;在Unix/Linux下，`multiprocessing`模块封装了`fork()`调用，使我们不需要关注
`fork()`的细节。由于Windows没有fork调用，因此，`multiprocessing`需要“模拟”出fork的效果，
父进程所有Python对象都必须通过`pickle`序列化再传到子进程去，所以，如果`multiprocessing`在
Windows下调用失败了，要先考虑是不是`pickle`失败了。